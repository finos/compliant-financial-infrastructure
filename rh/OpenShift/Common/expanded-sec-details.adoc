= OpenShift Extended Configuration details

== Overview

This documentation is provided as an addendum to the service accelerator that is part of the standard deployment and configuration of OpenShift.


=== *Controls and Architectures*

The information provided in the table below is compiled from product
documentation, blog posts, white papers and other resources and
emphasize recommended configurations that ensure Red Hat OpenShift
Security and Platform Integrity.


|===
|[big]*Security Domain*|[big]*Control & Architectural Suggestions*|[big]*References*

|[big]*Authentication & Authorization*
|
|
|*Understanding Authentication*
a|There are several Related to Authentication and Authorization -
Users - The primary entity that interacts with the API Server, Assign permissions to the user by adding roles to the user or groups the user belongs to
Identity - Keeps a record of successful auth attempts from a specific user and identity provider.  All data concerning the source of the authentication is stored on the provider.
Service Account - Applications can directly communicate with the API, Service accounts are used in place of sharing user accouts.
Groups - Groups contain a set of specific users, users can belong to multiple groups. Permissions can be assigned to multiple users via groups.

Users are allowed interaction with OpenShift via the Authentication and
Authorization Layers, A user makes a request to the API, and the
authentication layer authenticates the user. The authorization layer
uses RBAC to determin privileges.

.There are two authentication methods:
. *OAuth Access Tokens*
. *X.509 Client Certificates*

The Authentication Operator runs an OAuth Service which provide the
access tokens to users when they attempt to authenticate to the API. The
OAuth Server uses an identity provider to validate the request.
OpenShift creates the identity and user resources after a successful
login.

Configurable Identity providers include: HTPAsswd. Keystone, LDAP,
GitHub or GitHub Enterprise, OpenID Connect, Google, GitLab and Basic
Authentication.

By default the OpenShift Container Platform creates a cluster
administrator kubeadmin after installation which should be removed once
authentication is configured.

https://docs.openshift.com/container-platform/4.9/authentication/remove-kubeadmin.html[https://docs.openshift.com/container-platform/4.9/authentication/remove-kubeadmin.html]

----
$ oc delete secrets kubeadmin -n kube-system
----
{sp} +


a|

* Understanding authentication: https://docs.openshift.com/container-platform/4.9/authentication/understanding-authentication.html
* Orgs Management and Team Onboarding in OpenShift: https://www.openshift.com/blog/orgs-management-and-team-onboarding-in-openshift-a-fully-automated-approach
* Removing the kubeadmin user: https://docs.openshift.com/container-platform/4.9/authentication/remove-kubeadmin.html


|*Understanding Authorization?*

a|After authentication, the OpenShift API request is passed along (with the asserted User info) to the Kubernetes authorization layer (after a visit to the Audit layer). This layer is responsible for ensuring that the user has been granted permissions, by policy, to perform the requested action against the requested resource. Although the Kubernetes authorization layer is pluggable, OpenShift does not allow customization here, and only uses the Role-Based Access Control (RBAC) authorization type
Authorization is handled by rules, roles and bindings.

* Rules - Set of permitted verbs on a set of objects. For example, whether a user or service account can create pods.

* Roles - Collections of rules. You can associate, or bind, users and groups to multiple roles.

* Bindings - Associations between users and/or groups with a role.

a|

* Using RBAC to define and apply permissions: https://docs.openshift.com/container-platform/4.9/authentication/using-rbac.html

|*Handling RBAC*
a|Authorization in OpenShift is managed using role-based access control (RBAC). OpenShift takes a deny-by-default approach to RBAC. There are two types of RBACS within OpenShift that determine which actions RBAC can authorize, Cluster and Local.

* Cluster RBAC - give Users or Groups the ability to manage the OpenShift Cluster
* Local RBAC - Users or Groups that are managing objects and attributes at the project level

---
{sp} +

.Default Cluster Roles available in OpenShift:
* admin - Can Manage All project Resources
* basic-user - read access to the project
* cluster-admin - Users with this role have access to the cluster resources. These users have full control of the cluster.
* cluster-statue - this role grants the ability to get status information
* edit - create, edit, change and delete common application resources from the project
* self-provisioner - this role allows the creation of new projects (cluster role not a project level role)
* view - Users with this role can view project resources.



a|
* Using RBAC to define and apply permissions: https://docs.openshift.com/container-platform/4.9/authentication/using-rbac.html

* How to customize OpenShift RBAC permissions: https://developers.redhat.com/blog/2017/12/04/customize-openshift-rbac-permissions/

|*Handling Privileged Access Management? (Privileged pod)*
a|OpenShift can use Security Context Constraints to control permissions for pods. These permissions include actions that a pod, a collection of containers, can perform and what resources it can access. You can use SCCs to define a set of conditions that a pod must run with in order to be accepted into the system.

{sp}+

.*SCCs allow an administrator to control:*

* Whether a pod can run privileged containers.
* The capabilities that a container can request.
* The use of host directories as volumes.
* The container user ID.
* The SELinux context of the container.
* The allocation of an FSGroup that owns the pod’s volumes.
* The use of host namespaces and networking.
* The configuration of allowable supplemental groups.
* Whether a container requires the use of a read only root file system.
* The usage of volume types.
* The configuration of allowable seccomp profiles.
a|
* Managing security context constraints: https://docs.openshift.com/container-platform/4.9/authentication/managing-security-context-constraints.html

* Managing SCCs in OpenShift: https://www.openshift.com/blog/managing-sccs-in-openshift

* Introduction to Security Contexts and SCCs: https://www.openshift.com/blog/introduction-to-security-contexts-and-sccs

| *Handling Privileged Access Management? (Node Access & Configuation)*
| Accessing Nodes directly

RHCOS is designed to be as immutable as possible, allowing for only a few system settings to be changed. These settings are configured remotely, with the help of a specific operator developed by OpenShift. This scenario means no user will need to access a node directly, and any changes to the node will need to be directly authorized through the use of the Red Hat Machine Operator.

Additionally, Node Tuning Operator helps you manage node-level tuning by orchestrating the Tuned daemon. The majority of high-performance applications require some level of kernel tuning. The Node Tuning Operator provides a unified management interface to users of node-level sysctls and more flexibility to add custom tuning specified by user needs.

The node can still be accessed via ssh for limited troubleshooting requirements.
a|
* Machine Config Operator: https://docs.openshift.com/container-platform/4.9/post_installation_configuration/machine-configuration-tasks.html#understanding-the-machine-config-operator
* Tuning Operator: https://docs.openshift.com/container-platform/4.9/nodes/nodes/nodes-node-tuning-operator.html
* Manually gathering logs & troubleshooting - https://docs.openshift.com/container-platform/4.9/installing/installing-troubleshooting.html#installation-manually-gathering-logs-with-SSH_installing-troubleshooting



|*Security Monitoring & Alerting*
|
|
|*OpenShift Security Approach*
|The security tooling provided and inherent in the platform encourages the utilization of security as a fluid methodology strengthening each layer of the platform and each stage of the application delivery lifecycle.
a|
* A layered approach to container and Kubernetes security: https://www.redhat.com/en/resources/layered-approach-security-detail

|*Examining Auditing Capabilities*
a| In OpenShift Container Platform, auditing occurs at both a host operating system context and at an OpenShift API context.

Auditing of the host operating system consists of the standard auditing capabilities provided by the auditd service in *Red Hat Enterprise Linux
(RHEL)* and *Red Hat CoreOS (RHCOS)*. Audit is enabled by default in Red Hat Enterprise Linux CoreOS (RHCOS); however, the audit subsystem is running in a default configuration and without any audit rules. The auditd configuration ( /etc/audit/auditd.conf ) file should be modified as necessary to meet common organizational audit requirements such as retention and fault tolerance. Additionally, audit rules must be configured to record events.

Auditing at the OpenShift context consists of recording the HTTP requests made to the OpenShift API. The OpenShift API consists of two
components:

. *The Kubernetes API server*
. *The OpenShift API server*

Both of these components provide an audit log, each recording the events that
have affected the system by individual users, administrators, or other components of the system. OpenShift API audit is enabled by default and is produced by both the kube-apiserver and openshift-apiserver components. The audit configuration of each is defined by a combination of default settings and corresponding custom resources named KubeAPIServer and OpenShiftAPIServer, respectively. For more information, consult the Kubernetes Auditing documentation https://kubernetes.io/docs/tasks/debug-application-cluster/audit/.

'''


1 - Edit the APIServer resource:
----
$ oc edit apiserver cluster
----

2 - Update the spec.audit.profile field:

----
apiVersion: config.openshift.io/v1
  kind: APIServer
  metadata:
  ...
  spec:
    audit:
      profile: WriteRequestBodies
----

3 - Save the file to apply the changes.

4 - Verify that a new revision of the Kubernetes API server pods has rolled out. This will take several minutes.
----
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'


----
{sp} +



a|
* Viewing audit logs: https://docs.openshift.com/container-platform/4.9/security/audit-log-view.html#audit-log-view

* Configuring the audit log policy: https://docs.openshift.com/container-platform/4.9/security/audit-log-policy-config.html

* Auditing the OS: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/auditing-the-system_security-hardening

a|*Enforcing Compliance*
a|The Compliance Operator lets OpenShift Container Platform administrators describe the desired compliance state of a cluster and provides them with an overview of gaps and ways to remediate them. The Compliance Operator assesses compliance of both the Kubernetes API resources of OpenShift Container Platform, as well as the nodes running the cluster. The Compliance Operator uses OpenSCAP, a NIST-certified tool, to scan and enforce security policies provided by the content. Currently the following profiles are available for Compliance:

----
$ oc get -n profiles.compliance
NAME
ocp4-cis
ocp4-cis-node
ocp4-e8
ocp4-moderate
ocp4-ncp
rhcos4-e8
rhcos4-moderate
rhcos4-ncp
----
{sp} +

a|
* Understanding the Compliance Operator: https://docs.openshift.com/container-platform/4.9/security/compliance_operator/compliance-operator-understanding.html

* How does Compliance Operator work for OpenShift?: https://www.openshift.com/blog/how-does-compliance-operator-work-for-openshift-part-1

* RHEL CoreOS Compliance Scanning in OpenShift 4: https://www.openshift.com/blog/rhel-coreos-compliance-scanning-in-openshift-4

|*Enforcing File Integrity & Intrusion Detection*
| The File Integrity Operator is an OpenShift Container Platform Operator that continually runs file integrity checks on the cluster nodes. It deploys a daemon set that initializes and runs privileged advanced intrusion detection environment (AIDE) containers on each node, providing a status object with a log of files that are modified during the initial run of the daemon set pods.
a|
* Understanding the File Integrity Operator: https://docs.openshift.com/container-platform/4.9/security/file_integrity_operator/file-integrity-operator-understanding.html

* Configuring the Custom File Integrity Operator: https://docs.openshift.com/container-platform/4.9/security/file_integrity_operator/file-integrity-operator-configuring.html

* How to install and use the File Integrity Operator in Red Hat OpenShift Container Platform 4.9: https://access.redhat.com/solutions/5751261

| *Configuring Alerting*
a| In OpenShift Container Platform 4.9, the Alerting UI enables you to manage alerts, silences, and alerting rules.

* Alerting rules - Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.

* Alerts - An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an OpenShift Container Platform cluster.

* Silences - A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.

a|

* Configuring alert notifications: https://docs.openshift.com/container-platform/4.9/post_installation_configuration/configuring-alert-notifications.html
* Managing alerts: https://docs.openshift.com/container-platform/4.9/monitoring/managing-alerts.html
* Understanding cluster logging alerts: https://docs.openshift.com/container-platform/4.9/logging/troubleshooting/cluster-logging-alerts.html


| *Monitoring OpenShift*
a| OpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components.
OpenShift Container Platform delivers monitoring best practices out of the box. A set of alerts are included by default that immediately notify cluster administrators about issues with a cluster. Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster.

After installing OpenShift Container Platform 4.9, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects. You can then query metrics, review dashboards, and manage alerting rules and silences for your own projects in the OpenShift Container Platform web console.

a|
Understanding the monitoring stack: https://docs.openshift.com/container-platform/4.9/monitoring/understanding-the-monitoring-stack.html

Configuring the monitoring stack: https://docs.openshift.com/container-platform/4.9/monitoring/configuring-the-monitoring-stack.html

|*OpenShift Alerting*
| Alerting is built into the platform. Alerts can be managed via rules, queried upon, and surfaced on a visual dashboard. Alerts can send notices to external systems
a|
Managing alerts: https://docs.openshift.com/container-platform/4.9/monitoring/managing-alerts.html

|[big]*Data Resilience (back-up/replication)*
|
a|

| *Backup Capabilities*
| Back up the OpenShift cluster’s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. Do not take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates. It is also recommended to take etcd backups during non-peak usage hours, as it is a blocking action.
a|
Backing up etcd: https://docs.openshift.com/container-platform/4.9/backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.html


| *OpenShift High Availability*
a| The standard OpenShift Architecture consists of 3 control plane nodes or masters and at least two worker nodes providing localized high availability in the event a master node or worker node is lost.

.For multi-site High Availability there are two ways to achieve this:

. Install complete clusters across Availability Zones or Sites and have applications deployed to multiple clusters with load balancing between the two separate clusters.
. Have an OpenShift Cluster span 3 sites with a master node in each site and workers distributed. This type of cluster is extremely sensitive to network and other external variables and extreme consideration and testing should be applied to the architecture and deployment.

a| OpenShift Container Platform architecture: https://docs.openshift.com/container-platform/4.9/architecture/architecture.html

Stretch and multi-site clusters Capabilities and Support: https://access.redhat.com/articles/3220991#policies



| *Ensuring the latest stable/secure version of the underlying software is being used*
a| OpenShift provides over the air updates to both the underlying RHCOS nodes as well as the cluster itself. The entire platform is treated as one composable platform. Updates are packed in containers and can be set to apply automatically from selected channels and releases.Cluster Upgrades are managed via the Cluster Version Operator, the Machine Config Operator and some individual Operators. Updates and Patches are managed by the Cluster Administrator. Updates to nodes are done in a rolling fashion ensuring zero cluster downtime for applications designed according to cloud native principals. ALL platform Operators ensure that any drift from unsupported configuration changes are reset to the baseline configuration.
RHCOS is tightly coupled with the platform in order to consistently apply OS Updates, the Machine Config Operator can apply upgrades automatically in a coordinated fashion, minimizing cluster impact. Updates are released with the OpenShift Cluster update payload ensuring OS releases are in sync with Cluster releases.

OpenShift Updates can bee applied via the Web Console or CLI -

From the Web Console the user will be notified if the update is available, from there they can simply click update.

.From the command line, there are a few steps:

. Check if the cluster is available - oc get clusterversion
. Check if an update is available - oc adm upgrade
. Apply an update to the latest release - oc adm upgrade --to-latest=true
a| * Updating a cluster within a minor version by using the CLI : https://docs.openshift.com/container-platform/4.9/updating/updating-cluster-cli.html

|*Managing the underlying Container Host Operating System*
a|As mentioned in the latest version category the operating system (Red Hat CoreOS Operating System) and OpenShift kubernetes orchestration platform are tightly coupled together to ensure consistency and interoperability between fast moving components. Since RHCOS is only intended to be used by Red Hat OpenShift, it's installable via the:

*  OpenShift Installer Provisioned Infrastructure (IPI)

or

*  User Provisioned Infrastructure (UPI) _(the user is responsible to downloading the image and generating the ingnition control scripts.)_


The RHCOS Operating System is designed to be a single purpose container Operating System only supported in the capacity of OpenShift Container Platform usage which allows it to be more targeted and controlled than general purpose Operating Systems. It's based on Red Hat Enterprise Linux and inherits all the of the security and hardware certifications of RHEL in addition to the secure and stable OS Lifecycle. Reiterating the single use, the OS lacks non-critical components generally found in multi-purpose Operating Systems, which greatly reduce the attack surface. The state of the Operating System is stored within the OpenShift Container Platform ensuring controlled immutability, allowing the nodes to be scaled in either direction.

The container Runtime is CRI-O which is designed to be specifically used with Kubernetes implementing only the features needed, again minimizing the attack surface.

The last two capabilities to highlight are the way in which software is updated using rpm-ostree and the way RHCOS is configured using the Machine Config Operator. RPM-OSTREE features transactional upgrades. Updates are delivered by way of a container as part of the OpenShift upgrade process and extracted to disk. From there the bootloader is modified to boot into the updated version. There is the ability to rollback as neccessary. The Machine Config Operator handles the OS upgrades directed using rpm-ostree as well as maintaining and applying node configurations. This state is maintained accross all cluster nodes.

Please see the associated links for RHCOS Configuration, Hardening, Compliance Scanning and Installation.
a|
* Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines: https://docs.openshift.com/container-platform/4.9/installing/installing_bare_metal/installing-bare-metal.html#creating-machines-bare-metal

* RHEL CoreOS Compliance Scanning in OpenShift 4: https://www.openshift.com/blog/rhel-coreos-compliance-scanning-in-openshift-4

* Hardening RHCOS: https://docs.openshift.com/container-platform/4.9/security/container_security/security-hardening.html

|*Secrets & Key Management*
a|The Secret object type provides a mechanism to hold sensitive information such as passwords, OpenShift Container Platform client configuration files, private source repository credentials, and so on. Secrets decouple sensitive content from the pods. You can mount secrets into containers using a volume plug-in or the system can use secrets to perform actions on behalf of a pod.

.Key properties include:

* Secret data can be referenced independently from its definition.

* Secret data volumes are backed by temporary file-storage facilities (tmpfs) and never come to rest on a node.

* Secret data can be shared within a namespace.
a|* Storing Sensitive Data: https://docs.openshift.com/container-platform/4.9/nodes/pods/nodes-pods-secrets.html

|*Handling Certificate Management?*
a|All certificates for internal traffic is managed by OpenShift and rotated automatically. Egress (Proxy) traffic CA is configurable. Ingres traffic is configurable

Service serving certificates are intended to support complex middleware applications that require encryption. These certificates are issued as TLS web server certificates.

The service-ca controller uses the x509.SHA256WithRSA signature algorithm to generate service certificates.

The generated certificate and key are in PEM format, stored in tls.crt and tls.key respectively, within a created secret. The certificate and key are automatically replaced when they get close to expiration.

The service CA certificate, which issues the service certificates, is valid for 26 months and is automatically rotated when there is less than six months validity left. After rotation, the previous service CA configuration is still trusted until its expiration. This allows a grace period for all affected services to refresh their key material before the expiration. If you do not upgrade your cluster during this grace period, which restarts services and refreshes their key material, you might need to manually restart services to avoid failures after the previous service CA expires.

a|* Certificate Management: https://docs.openshift.com/container-platform/4.9/security/certificates/service-serving-certificate.html


|*Encryption*
a| By default, etcd data is not encrypted in OpenShift Container Platform. You can enable etcd encryption for your cluster to provide an additional layer of data security. For example, it can help protect the loss of sensitive data if an etcd backup is exposed to the incorrect parties.

.When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:

* Secrets

* Config maps

* Routes

* OAuth access tokens

* OAuth authorize tokens

When you enable etcd encryption, encryption keys are created. These keys are rotated on a weekly basis. *You must have these keys in order to restore from an etcd backup.*
a|
* Encrypting ETCD: https://docs.openshift.com/container-platform/4.9/security/encrypting-etcd.html

|*Evaluating handle Encryption in Transit?*
| With IPsec enabled, all network traffic between nodes on the OVN-Kubernetes Container Network Interface (CNI) cluster network travels through an encrypted tunnel.
a|
* IPsec encryption configuration: https://docs.openshift.com/container-platform/4.9/networking/ovn_kubernetes_network_provider/about-ipsec-ovn.html

|*Network Policy and Security*
|In a cluster using a Kubernetes Container Network Interface (CNI) plug-in that supports Kubernetes network policy, network isolation is controlled entirely by NetworkPolicy objects. In OpenShift Container Platform 4.9, OpenShift SDN supports using network policy in its default network isolation mode.

By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create *NetworkPolicy* objects in that project to indicate the allowed incoming connections. Project administrators can create and delete NetworkPolicy objects within their own project.

If a pod is matched by selectors in one or more NetworkPolicy objects, then the pod will accept only connections that are allowed by at least one of those NetworkPolicy objects. A pod that is not selected by any NetworkPolicy objects is fully accessible.
a|* NetworkPolicy Configuration and Details: https://docs.openshift.com/container-platform/4.9/networking/network_policy/about-network-policy.html#nw-networkpolicy-about_about-network-policy
* OpenShift Networking and Cluster Access Best Practices: https://www.openshift.com/blog/openshift-networking-and-cluster-access-best-practices

|===
